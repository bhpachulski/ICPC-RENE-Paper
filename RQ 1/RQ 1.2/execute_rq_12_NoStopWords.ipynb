{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import arff\n",
    "from io import StringIO\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def weka_tokenizer(doc):\n",
    "    delimiters_regexp = re.compile(\"[ |\\n|\\f|\\r|\\t|.|,|;|:|'|\\\"|(|)|?|!]\")\n",
    "    return list(filter(None, delimiters_regexp.split(doc)))\n",
    "\n",
    "def initClassifiers():\n",
    "    classifiers = {\n",
    "        'randomForest': RandomForestClassifier(random_state=1), \n",
    "        'decisionTree': DecisionTreeClassifier(min_samples_leaf=1),\n",
    "        'naiveBayes': GaussianNB(),\n",
    "        'smo': CalibratedClassifierCV(LinearSVC(fit_intercept=False, tol=0.001, C=1, dual=False, max_iter=100000), method='sigmoid'),\n",
    "        'knn': KNeighborsClassifier(n_neighbors=1, metric='euclidean'),\n",
    "        'logisticRegression': LogisticRegression(max_iter=1000),\n",
    "        'perceptron': CalibratedClassifierCV(Perceptron()),\n",
    "        'lda': LinearDiscriminantAnalysis(),\n",
    "    }\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "def round_float(value):\n",
    "    return float(\"{:.3f}\".format(value))\n",
    "\n",
    "def execClassifiers(X_train, x_test, y_train, y_test, classifiers):\n",
    "\n",
    "    labels = ['Flaky', 'NonFlaky']\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    comparison_values = {}\n",
    "\n",
    "    # create a normalized version\n",
    "    trainScaler = Binarizer(threshold=0.0).fit(X_train)\n",
    "    testScaler = Binarizer(threshold=0.0).fit(x_test)\n",
    "    X_train_norm = trainScaler.transform(X_train)\n",
    "    x_test_norm = testScaler.transform(x_test)\n",
    "\n",
    "    for key, classifier in classifiers.items():\n",
    "\n",
    "        x_train_exec = X_train\n",
    "        x_test_exec = x_test\n",
    "        y_train_exec = y_train\n",
    "        y_test_exec = y_test        \n",
    "\n",
    "        classifier.fit(x_train_exec, y_train)\n",
    "        classifier.score(x_test_exec, y_test)\n",
    "\n",
    "        predict = classifier.predict(x_test_exec)\n",
    "        y_probs = classifier.predict_proba(x_test_exec)[:,1]\n",
    "\n",
    "        result = {\n",
    "            'classifier': key,\n",
    "            'f1Score': f1_score(y_test, predict, average='weighted'), #labels=labels,\n",
    "            'accuracy': classifier.score(x_test_exec, y_test),\n",
    "            'confucionMatrix': confusion_matrix(y_test, predict),\n",
    "            'classificationReport': classification_report(y_test, predict, output_dict=True), #, target_names=labels\n",
    "            'AUC': roc_auc_score(y_test, y_probs),\n",
    "            'MCC': matthews_corrcoef(y_test, predict), \n",
    "        }\n",
    "\n",
    "        results = results.append(result,  ignore_index=True)    \n",
    "                        \n",
    "        print(key, classification_report(y_test, predict, output_dict=True)['Flakey'], matthews_corrcoef(y_test, predict), roc_auc_score(y_test, y_probs), \"\\n \\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = arff.load('../../datasets/MSR4FlakinessOriginal.arff')\n",
    "\n",
    "completeColumns = ['tokens', 'loc', 'abstract_keyword', 'assert_keyword', 'boolean_keyword', 'break_keyword', 'byte_keyword', 'case_keyword', 'catch_keyword', 'char_keyword', 'class_keyword', 'continue_keyword', 'default_keyword', 'do_keyword', 'double_keyword', 'else_keyword', 'enum_keyword', 'exports_keyword', 'extends_keyword', 'final_keyword', 'finally_keyword', 'float_keyword', 'for_keyword', 'if_keyword', 'implements_keyword', 'import_keyword', 'instanceof_keyword', 'int_keyword', 'interface_keyword', 'long_keyword', 'modules_keyword', 'native_keyword', 'new_keyword', 'package_keyword', 'private_keyword', 'protected_keyword', 'public_keyword', 'requires_keyword', 'return_keyword', 'short_keyword', 'static_keyword', 'strictfp_keyword', 'super_keyword', 'switch_keyword', 'synchronized_keyword', 'this_keyword', 'throw_keyword', 'throws_keyword', 'transient_keyword', 'try_keyword', 'void_keyword', 'volatile_keyword', 'while_keyword', 'true_keyword', 'null_keyword', 'false_keyword', 'const_keyword', 'goto_keyword', 'keywordcount', 'klass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete\n",
    "columns = completeColumns\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns) \n",
    "y = df['klass']\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=1500, tokenizer=weka_tokenizer) \n",
    "\n",
    "bowToken = vectorizer.fit_transform(df['tokens'])\n",
    "\n",
    "bowData = pd.DataFrame(bowToken.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "df.drop('tokens', axis=1, inplace=True)\n",
    "df = df.join(bowData)\n",
    "df.drop('klass', axis=1, inplace=True)\n",
    "x = df\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1) #, random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "randomForest {'precision': 0.9755102040816327, 'recall': 0.8884758364312267, 'f1-score': 0.9299610894941635, 'support': 269} 0.8742278288731938 0.9810625350104395 \n",
      " \n",
      "\n",
      "decisionTree {'precision': 0.890625, 'recall': 0.8475836431226765, 'f1-score': 0.8685714285714285, 'support': 269} 0.7539426357152474 0.8758466160818863 \n",
      " \n",
      "\n",
      "naiveBayes {'precision': 0.9497907949790795, 'recall': 0.8438661710037175, 'f1-score': 0.8937007874015749, 'support': 269} 0.8110207493991364 0.9024418190151244 \n",
      " \n",
      "\n",
      "smo {'precision': 0.9314516129032258, 'recall': 0.8587360594795539, 'f1-score': 0.8936170212765957, 'support': 269} 0.8052672769464404 0.96167948260936 \n",
      " \n",
      "\n",
      "knn {'precision': 0.8181818181818182, 'recall': 0.8698884758364313, 'f1-score': 0.8432432432432433, 'support': 269} 0.6913575419434269 0.8459031420278046 \n",
      " \n",
      "\n",
      "logisticRegression {'precision': 0.914179104477612, 'recall': 0.9107806691449815, 'f1-score': 0.9124767225325886, 'support': 269} 0.8321406840542867 0.963576411875541 \n",
      " \n",
      "\n",
      "perceptron {'precision': 0.9367588932806324, 'recall': 0.8810408921933085, 'f1-score': 0.9080459770114943, 'support': 269} 0.8295479840294974 0.9582675561440139 \n",
      " \n",
      "\n",
      "lda {'precision': 0.8365758754863813, 'recall': 0.7992565055762082, 'f1-score': 0.8174904942965779, 'support': 269} 0.6571802315422431 0.8669475989204053 \n",
      " \n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ccdd6aedf10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitClassifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecClassifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3cfe03359064>\u001b[0m in \u001b[0;36mexecClassifiers\u001b[0;34m(X_train, x_test, y_train, y_test, classifiers)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flakey'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mplot_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparison_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "classifiers = initClassifiers()\n",
    "results = execClassifiers(X_train, x_test, y_train, y_test, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import arff\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "analisysData = []\n",
    "\n",
    "def weka_tokenizer(doc):\n",
    "    delimiters_regexp = re.compile(\"[ |\\n|\\f|\\r|\\t|.|,|;|:|'|\\\"|(|)|?|!]\")\n",
    "    return list(filter(None, delimiters_regexp.split(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyFile = '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict'\n",
    "testDataset = '../../datasets/flakies_rq_22.csv'\n",
    "\n",
    "X_idFlakiesDataset = pd.read_csv(testDataset)\n",
    "y_idFlakiesDataset = X_idFlakiesDataset['klass']\n",
    "\n",
    "vocabulary = pickle.load(open(vocabularyFile, 'rb'))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=1500, tokenizer=weka_tokenizer, vocabulary=vocabulary) \n",
    "bowToken = vectorizer.fit_transform(X_idFlakiesDataset['tokens'])\n",
    "bowData = pd.DataFrame(bowToken.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "X_idFlakiesDataset.drop('tokens', axis=1, inplace=True)\n",
    "X_idFlakiesDataset = X_idFlakiesDataset.join(bowData)\n",
    "X_idFlakiesDataset.drop('klass', axis=1, inplace=True)\n",
    "X_idFlakiesDataset.drop('project', axis=1, inplace=True)\n",
    "\n",
    "# y_probs = classifier.predict_proba(X_idFlakiesDataset)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "\n",
    "for algSav in os.listdir(\"../../RQ 1/RQ 1.1/classifiers/\"):\n",
    "    classifiers[algSav.split('.')[0]] = pickle.load(open(\"../../RQ 1/RQ 1.1/classifiers/\" + algSav, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test dataset: ../../datasets/flakies_rq_22.csv\n",
      "Vocabulary: ../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict\n",
      "256 flakie samples\n",
      "1500 features \n",
      "\n",
      "   -  naiveBayes {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'naiveBayes', 'Precision': 1.0, 'Recall': 0.09375, 'F1-Score': 0.17142857142857143, 'Support': 256, 'Accuracy': 0.09375, 'MCC': 0.0, 'VP': '', 'FN': '24 232'}\n",
      "   -  knn {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'knn', 'Precision': 1.0, 'Recall': 0.0859375, 'F1-Score': 0.15827338129496402, 'Support': 256, 'Accuracy': 0.0859375, 'MCC': 0.0, 'VP': '', 'FN': '22 234'}\n",
      "   -  logisticRegression {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'logisticRegression', 'Precision': 1.0, 'Recall': 0.3984375, 'F1-Score': 0.5698324022346368, 'Support': 256, 'Accuracy': 0.3984375, 'MCC': 0.0, 'VP': '102', 'FN': '154'}\n",
      "   -  perceptron {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'perceptron', 'Precision': 1.0, 'Recall': 0.37890625, 'F1-Score': 0.5495750708215298, 'Support': 256, 'Accuracy': 0.37890625, 'MCC': 0.0, 'VP': '', 'FN': '97 159'}\n",
      "   -  decisionTree {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'decisionTree', 'Precision': 1.0, 'Recall': 0.15625, 'F1-Score': 0.2702702702702703, 'Support': 256, 'Accuracy': 0.15625, 'MCC': 0.0, 'VP': '', 'FN': '40 216'}\n",
      "   -  smo {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'smo', 'Precision': 1.0, 'Recall': 0.11328125, 'F1-Score': 0.20350877192982456, 'Support': 256, 'Accuracy': 0.11328125, 'MCC': 0.0, 'VP': '', 'FN': '29 227'}\n",
      "   -  randomForest {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'randomForest', 'Precision': 1.0, 'Recall': 0.015625, 'F1-Score': 0.03076923076923077, 'Support': 256, 'Accuracy': 0.015625, 'MCC': 0.0, 'VP': '', 'FN': ' 4 252'}\n",
      "   -  lda {'Validation Dataset': '../../datasets/flakies_rq_22.csv', 'Vocabulary': '../../RQ 1/RQ 1.1/vocabulary/MSR4FlakinessOriginal.dict', 'Classifier': 'lda', 'Precision': 1.0, 'Recall': 0.5078125, 'F1-Score': 0.6735751295336787, 'Support': 256, 'Accuracy': 0.5078125, 'MCC': 0.0, 'VP': '130', 'FN': '126'}\n"
     ]
    }
   ],
   "source": [
    "print('Test dataset:', testDataset)\n",
    "print('Vocabulary:', vocabularyFile)\n",
    "print(len(y_idFlakiesDataset), \"flakie samples\")\n",
    "print(len(vectorizer.vocabulary), \"features \\n\")\n",
    "\n",
    "for keyClassifier, classifier in classifiers.items():\n",
    "\n",
    "    predict = classifier.predict(X_idFlakiesDataset)\n",
    "\n",
    "    classificationReport = classification_report(y_idFlakiesDataset, predict, output_dict=True)['Flakey']\n",
    "    classifierScore = classifier.score(X_idFlakiesDataset, y_idFlakiesDataset)      \n",
    "\n",
    "    cmStr = confusion_matrix(y_idFlakiesDataset, predict)[0]\n",
    "\n",
    "    cm = re.sub('\\[|\\]', '', str(cmStr))\n",
    "\n",
    "    VP = cm.split(\" \", 1)[0] \n",
    "    FN = cm.split(\" \", 1)[1] \n",
    "\n",
    "    classifierResult = {\n",
    "        'Validation Dataset': testDataset, \n",
    "        'Vocabulary': vocabularyFile, \n",
    "        'Classifier': keyClassifier, \n",
    "        'Precision': classificationReport['precision'],\t\n",
    "        'Recall': classificationReport['recall'], \n",
    "        'F1-Score': classificationReport['f1-score'], \n",
    "        'Support': classificationReport['support'],\n",
    "        'Accuracy': classifierScore,\n",
    "        # 'AUC': roc_auc_score(y_idFlakiesDataset, y_probs),\n",
    "        'MCC': matthews_corrcoef(y_idFlakiesDataset, predict),\n",
    "        'VP': VP,\n",
    "        'FN': FN\n",
    "    }\n",
    "\n",
    "    print('   - ', keyClassifier, classifierResult)\n",
    "\n",
    "    analisysData.append(classifierResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisysDataCSV = pd.DataFrame(analisysData, columns=['Validation Dataset', 'Vocabulary', 'Training Dataset', 'Classifier', 'Precision',\t'Recall', 'F1-Score', 'Support', 'Accuracy', 'AUC', 'MCC', 'VP', 'FN'])\n",
    "analisysDataCSV.to_csv(\"execution_rq_22.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}